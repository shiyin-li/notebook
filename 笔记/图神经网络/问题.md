#### 1.不太理解 咋的出来的

![image-20230406110700404](../../assets/问题/image-20230406110700404.png)

![image-20230406110607918](../../assets/问题/image-20230406110607918.png)

#### 2.半监督学习 

少量有标签学习样本 有监督训练出来 model没有半监督学习好 

大量无标签学习样本

#### 3.极大似然估计  ->5

![image-20230411105012139](../../assets/问题/image-20230411105012139.png)

#### 4.两种随机梯度下降的计算方法

![image-20230411112401289](../../assets/问题/image-20230411112401289.png)

![image-20230411112513452](../../assets/问题/image-20230411112513452.png)

相当于自己学的三种

批量梯度下降 速度慢 但是准 batch

**随机梯度下降 快  但不一定准 次数多也能达到准确？** 

小批量梯度下降  两者的折中 mini-batch

#### 5.node2vec构建的向量

![image-20230411152127150](../../assets/问题/image-20230411152127150.png)

#### 6.L1 L2范数

怎么分解矩阵 又如何优化 菲罗贝尼乌斯

![image-20230411155327558](../../assets/问题/image-20230411155327558.png) 

#### 7.匿名随机游走的另一种策略 这种是自己学习吗？

![image-20230411164043881](../../assets/问题/image-20230411164043881.png)

![image-20230411163930727](../../assets/问题/image-20230411163930727.png)

#### 8.这个算法是如何优化每个d维向量的

**匿名随机游走的另一种策略**

![image-20230411164059955](../../assets/问题/image-20230411164059955.png)

![image-20230411163930727](../../assets/问题/image-20230411163930727-1681203739974-2.png)

![image-20230411164423958](../../assets/问题/image-20230411164423958.png)

##### 为何要构建这个任务（全图WG  w1 w2 w3去预测w4）因为要构建一个子监督学习任务

![image-20230411164844976](../../assets/问题/image-20230411164844976.png)

``这是一个极大似然估计的自监督学习任务  自己迭代跟新优化`` 

#### 9.word2vec 算法 还有论文 要看 

#### 10. 为何可以用先验概率  后验概率  再加似然估计来建立model  例如：language model 用句子前n-1单词来预测第n个单词（和贝叶斯有相似之处 用已知来推未知）

#### 11.skip-gram 中间词预测周围词 周围词预测中间词 完形填空；skip-gram 中间词预测周围词 。 这两个自监督学习是如何更新的，即算法原理

#### 12.node2vec中dfs 和bfs下面的图所举例子对吗？

![image-20230413103614679](../../assets/问题/image-20230413103614679.png)

#### 13.pagerank基于random walk的理解中 p(t)向量是什么意思 举个例子？

![image-20230414101202708](../../assets/问题/image-20230414101202708.png)

#### 14.PageRank的变种

**Q点随机游走 Q先到用户然后让用户随机访问商品最后再回到Q； 多次随机游走记录商品被访问的次数 由此可以得出和Q商品相似的商品 。这样理解对不？**

![image-20230414105407019](../../assets/问题/image-20230414105407019.png)

#### 15.神经网络如果不加激活函数就只能解决线性问题？

#### 16.这个求和  能举个例子吗？

![image-20230415173343514](../../assets/问题/image-20230415173343514.png)

![image-20230415172341717](../../assets/问题/image-20230415172341717.png)

![image-20230415173329914](../../assets/问题/image-20230415173329914.png)

H(k)是每个节点的第k层的embedding 的向量(矩阵更好一点 v个结点d维(假设第k层每个结点输出1维)) 列向量(因为是转置)

Nv是邻域结点 ；A是邻接矩阵；Av是v节点的邻域结点的embedding(行向量也当作一维)

一行乘以一列就是一个数   

这样理解对吗？

#### 17.这里的Dii是degree矩阵中的第i行i列个节点？

![image-20230416092003687](../../assets/问题/image-20230416092003687.png)

#### 18.deep walk 和node2vec 是查表更行表(matrix) 那这个表的初始值如何定义？

随机初始化 使用one-hot，然后将其变为低维连续稠密的向量

1. 低维：向量的维度通常很低，比如50维或100维，以便更容易计算和存储。
2. 连续：向量中的每个元素都是实数，且元素之间的距离可以表示它们之间的相似性。
3. 稠密：向量中的大多数元素都是非零的，这样可以提高表示的丰富性和表达能力。

#### 19.这个损失函数是啥。怎么更新迭代优化？极大似然估计？交叉熵？梯度下降？

![image-20230416094636124](../../assets/问题/image-20230416094636124.png)

#### 20.神经网络根据万能近似定理可以类似于任何连续函数 

**那么神经网络的隐藏层怎么条才能，输入一些东西得到我们想要的结果 是不断的调参？eg：一个简单的猫狗图片分类问题  他怎么得到真正的猫狗分类 中间的隐藏层到底是什么函数(我们也不知道  调参调出来？)**
