## 深度学习基础

![image-20230415092540035](../../assets/10子豪深度学习基础/image-20230415092540035.png)

![image-20230415092614322](../../assets/10子豪深度学习基础/image-20230415092614322.png)

![image-20230415092657824](../../assets/10子豪深度学习基础/image-20230415092657824.png)

![image-20230415092757270](../../assets/10子豪深度学习基础/image-20230415092757270.png)

![image-20230415092926055](../../assets/10子豪深度学习基础/image-20230415092926055.png)

![image-20230415093023230](../../assets/10子豪深度学习基础/image-20230415093023230.png)

![image-20230415093344192](../../assets/10子豪深度学习基础/image-20230415093344192.png)

![image-20230415093629684](../../assets/10子豪深度学习基础/image-20230415093629684.png)

![image-20230415093741766](../../assets/10子豪深度学习基础/image-20230415093741766.png)

![image-20230415093900186](../../assets/10子豪深度学习基础/image-20230415093900186.png)

![image-20230415093927406](../../assets/10子豪深度学习基础/image-20230415093927406.png)

- 损失函数就是预测值和实际值的偏差大小  优化损失函数就是使其偏差变得很小
- 对于回归分析 eg：线性回归使用L2范数(均方误) 便于求导和微分
- 对于分类问题 一般使用交叉熵 

![image-20230415094002986](../../assets/10子豪深度学习基础/image-20230415094002986.png)

![image-20230415094748534](../../assets/10子豪深度学习基础/image-20230415094748534.png)

![image-20230415094923215](../../assets/10子豪深度学习基础/image-20230415094923215.png)

![image-20230415094937536](../../assets/10子豪深度学习基础/image-20230415094937536.png)

![image-20230415095152803](../../assets/10子豪深度学习基础/image-20230415095152803.png)

![image-20230415095240201](../../assets/10子豪深度学习基础/image-20230415095240201.png)

**道路曲折 但是前途光明**

![image-20230415095255910](../../assets/10子豪深度学习基础/image-20230415095255910.png)

![image-20230415095357050](../../assets/10子豪深度学习基础/image-20230415095357050.png)

![image-20230415095521678](../../assets/10子豪深度学习基础/image-20230415095521678.png)

![image-20230415095626023](../../assets/10子豪深度学习基础/image-20230415095626023.png)

![image-20230415095737386](../../assets/10子豪深度学习基础/image-20230415095737386.png)

![image-20230415095920788](../../assets/10子豪深度学习基础/image-20230415095920788.png)

![image-20230415100035453](../../assets/10子豪深度学习基础/image-20230415100035453.png)

![image-20230415100104804](../../assets/10子豪深度学习基础/image-20230415100104804.png)

![image-20230415100450933](../../assets/10子豪深度学习基础/image-20230415100450933.png)

**不同的优化器：www.ilibili.com/video/BV1K7411W7So?p=8**

![image-20230415100633103](../../assets/10子豪深度学习基础/image-20230415100633103.png)

#### 什么叫线性变换？

![image-20230415101123347](../../assets/10子豪深度学习基础/image-20230415101123347.png)

![image-20230415101055742](../../assets/10子豪深度学习基础/image-20230415101055742.png)

![image-20230415101538510](../../assets/10子豪深度学习基础/image-20230415101538510.png)

![image-20230415101629620](../../assets/10子豪深度学习基础/image-20230415101629620.png)

![image-20230415101718165](../../assets/10子豪深度学习基础/image-20230415101718165.png)

##### 观察不同激活函数的神经网络

![image-20230415102256140](../../assets/10子豪深度学习基础/image-20230415102256140.png)

![image-20230415102719726](../../assets/10子豪深度学习基础/image-20230415102719726.png)

![image-20230415102943372](../../assets/10子豪深度学习基础/image-20230415102943372.png)

- Batch Noemalization：可以去掉偏置项 以更大的学习率来替代
- Dropout ：随即掐死部分神经元 每次迭代掐死的都不一样 可以防止overfitting
- Attention /Gating：注意力机制 门控机制



![image-20230415103414036](../../assets/10子豪深度学习基础/image-20230415103414036.png)



![image-20230415103442429](../../assets/10子豪深度学习基础/image-20230415103442429.png)

![image-20230415103459499](../../assets/10子豪深度学习基础/image-20230415103459499.png)